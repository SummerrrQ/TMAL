# Boosting Few-Shot Action Recognition via Time-enhanced Multimodal Adaptation Learning（TMAL）


![屏幕截图 2024-08-08 143408](https://github.com/user-attachments/assets/a1719c3a-66da-4666-9a72-e540e3a11324)
> **Boosting Few-Shot Action Recognition via Time-enhanced Multimodal Adaptation Learning**<br>
> Dan Liu · Qing Xia · Fanrong Meng · Ai Peng · Zhouli Shen · Mao Ye · Jianwei Zhang
>
>
<!-- > [Paper](https://arxiv.org/pdf/2303.02982.pdf) -->
>
>
> **Abstract:** *Few-shot action recognition (FSAR) aims to classify new classes with a limited number of labeled samples, which has garnered considerable attention due to the challenges of collecting and annotating large-scale video data. However, most of the existing methods mainly rely on limited unimodal visual data. Inspired by the success of multimodal machine learning, this paper introduces a straightforward yet effective multimodal adaptation learning framework for FSAR. Specifically, we transfer visual-text knowledge from the large pre-trained foundation model, such as CLIP, to acquire a generalized representation. Furthermore, temporal adaptation is implemented to adapt the image model to the video tasks by incorporating lightweight adapters. To enhance the temporal modeling for action analysis, a vision-motion aggregation technique is further proposed for comprehensive feature learning. Extensive experiments on multiple challenging benchmark datasets demonstrate that the suggested method achieves superior results in terms of accuracy and efficiency.*
>
>This code is based on [CLIP-FSAR](https://github.com/alibaba-mmai-research/CLIP-FSAR), [ST-Adapter](https://github.com/linziyi96/st-adapter) codebase, which  provids with innovative ideas in comprehensive video understanding for video classification and temporal modeling. 

As our paper is currently under review, we will promptly release the code once it is accepted...

